{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-05T09:45:16.439696Z",
     "start_time": "2018-09-05T09:45:16.398549Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, os, csv, platform, zipfile, datetime\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "DATA_ROOT = \"irb\"\n",
    "SESSION_ID = \"CHI2019\"\n",
    "\n",
    "def save_jsonfile(name, data):\n",
    "    with open(name, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "    print(\"File saved!\", name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T22:21:06.404799Z",
     "start_time": "2018-09-04T22:21:06.270818Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load lib/cvxEDA.py\n",
    "\"\"\"\n",
    "______________________________________________________________________________\n",
    "\n",
    " File:                         cvxEDA.py\n",
    " Last revised:                 07 Nov 2015 r69\n",
    " ______________________________________________________________________________\n",
    "\n",
    " Copyright (C) 2014-2015 Luca Citi, Alberto Greco\n",
    " \n",
    " This program is free software; you can redistribute it and/or modify it under\n",
    " the terms of the GNU General Public License as published by the Free Software\n",
    " Foundation; either version 3 of the License, or (at your option) any later\n",
    " version.\n",
    " \n",
    " This program is distributed in the hope that it will be useful, but WITHOUT\n",
    " ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n",
    " FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
    " \n",
    " You may contact the author by e-mail (lciti@ieee.org).\n",
    " ______________________________________________________________________________\n",
    "\n",
    " This method was first proposed in:\n",
    " A Greco, G Valenza, A Lanata, EP Scilingo, and L Citi\n",
    " \"cvxEDA: a Convex Optimization Approach to Electrodermal Activity Processing\"\n",
    " IEEE Transactions on Biomedical Engineering, 2015\n",
    " DOI: 10.1109/TBME.2015.2474131\n",
    "\n",
    " If you use this program in support of published research, please include a\n",
    " citation of the reference above. If you use this code in a software package,\n",
    " please explicitly inform the end users of this copyright notice and ask them\n",
    " to cite the reference above in their published research.\n",
    " ______________________________________________________________________________\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import cvxopt as cv\n",
    "import cvxopt.solvers\n",
    "\n",
    "def cvxEDA(y, delta, tau0=2., tau1=0.7, delta_knot=10., alpha=8e-4, gamma=1e-2,\n",
    "           solver=None, options={'reltol':1e-9}):\n",
    "    \"\"\"CVXEDA Convex optimization approach to electrodermal activity processing\n",
    "\n",
    "    This function implements the cvxEDA algorithm described in \"cvxEDA: a\n",
    "    Convex Optimization Approach to Electrodermal Activity Processing\"\n",
    "    (http://dx.doi.org/10.1109/TBME.2015.2474131, also available from the\n",
    "    authors' homepages).\n",
    "\n",
    "    Arguments:\n",
    "       y: observed EDA signal (we recommend normalizing it: y = zscore(y))\n",
    "       delta: sampling interval (in seconds) of y\n",
    "       tau0: slow time constant of the Bateman function\n",
    "       tau1: fast time constant of the Bateman function\n",
    "       delta_knot: time between knots of the tonic spline function\n",
    "       alpha: penalization for the sparse SMNA driver\n",
    "       gamma: penalization for the tonic spline coefficients\n",
    "       solver: sparse QP solver to be used, see cvxopt.solvers.qp\n",
    "       options: solver options, see:\n",
    "                http://cvxopt.org/userguide/coneprog.html#algorithm-parameters\n",
    "\n",
    "    Returns (see paper for details):\n",
    "       r: phasic component\n",
    "       p: sparse SMNA driver of phasic component\n",
    "       t: tonic component\n",
    "       l: coefficients of tonic spline\n",
    "       d: offset and slope of the linear drift term\n",
    "       e: model residuals\n",
    "       obj: value of objective function being minimized (eq 15 of paper)\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(y)\n",
    "    y = cv.matrix(y)\n",
    "\n",
    "    # bateman ARMA model\n",
    "    a1 = 1./min(tau1, tau0) # a1 > a0\n",
    "    a0 = 1./max(tau1, tau0)\n",
    "    ar = np.array([(a1*delta + 2.) * (a0*delta + 2.), 2.*a1*a0*delta**2 - 8.,\n",
    "        (a1*delta - 2.) * (a0*delta - 2.)]) / ((a1 - a0) * delta**2)\n",
    "    ma = np.array([1., 2., 1.])\n",
    "\n",
    "    # matrices for ARMA model\n",
    "    i = np.arange(2, n)\n",
    "    A = cv.spmatrix(np.tile(ar, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))\n",
    "    M = cv.spmatrix(np.tile(ma, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))\n",
    "\n",
    "    # spline\n",
    "    delta_knot_s = int(round(delta_knot / delta))\n",
    "    spl = np.r_[np.arange(1.,delta_knot_s), np.arange(delta_knot_s, 0., -1.)] # order 1\n",
    "    spl = np.convolve(spl, spl, 'full')\n",
    "    spl /= max(spl)\n",
    "    # matrix of spline regressors\n",
    "    i = np.c_[np.arange(-(len(spl)//2), (len(spl)+1)//2)] + np.r_[np.arange(0, n, delta_knot_s)]\n",
    "    nB = i.shape[1]\n",
    "    j = np.tile(np.arange(nB), (len(spl),1))\n",
    "    p = np.tile(spl, (nB,1)).T\n",
    "    valid = (i >= 0) & (i < n)\n",
    "    B = cv.spmatrix(p[valid], i[valid], j[valid])\n",
    "\n",
    "    # trend\n",
    "    C = cv.matrix(np.c_[np.ones(n), np.arange(1., n+1.)/n])\n",
    "    nC = C.size[1]\n",
    "\n",
    "    # Solve the problem:\n",
    "    # .5*(M*q + B*l + C*d - y)^2 + alpha*sum(A,1)*p + .5*gamma*l'*l\n",
    "    # s.t. A*q >= 0\n",
    "\n",
    "    old_options = cv.solvers.options.copy()\n",
    "    cv.solvers.options.clear()\n",
    "    cv.solvers.options.update(options)\n",
    "    if solver == 'conelp':\n",
    "        # Use conelp\n",
    "        z = lambda m,n: cv.spmatrix([],[],[],(m,n))\n",
    "        G = cv.sparse([[-A,z(2,n),M,z(nB+2,n)],[z(n+2,nC),C,z(nB+2,nC)],\n",
    "                    [z(n,1),-1,1,z(n+nB+2,1)],[z(2*n+2,1),-1,1,z(nB,1)],\n",
    "                    [z(n+2,nB),B,z(2,nB),cv.spmatrix(1.0, range(nB), range(nB))]])\n",
    "        h = cv.matrix([z(n,1),.5,.5,y,.5,.5,z(nB,1)])\n",
    "        c = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T,z(nC,1),1,gamma,z(nB,1)])\n",
    "        res = cv.solvers.conelp(c, G, h, dims={'l':n,'q':[n+2,nB+2],'s':[]})\n",
    "        obj = res['primal objective']\n",
    "    else:\n",
    "        # Use qp\n",
    "        Mt, Ct, Bt = M.T, C.T, B.T\n",
    "        H = cv.sparse([[Mt*M, Ct*M, Bt*M], [Mt*C, Ct*C, Bt*C], \n",
    "                    [Mt*B, Ct*B, Bt*B+gamma*cv.spmatrix(1.0, range(nB), range(nB))]])\n",
    "        f = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T - Mt*y,  -(Ct*y), -(Bt*y)])\n",
    "        res = cv.solvers.qp(H, f, cv.spmatrix(-A.V, A.I, A.J, (n,len(f))),\n",
    "                            cv.matrix(0., (n,1)), solver=solver)\n",
    "        obj = res['primal objective'] + .5 * (y.T * y)\n",
    "    cv.solvers.options.clear()\n",
    "    cv.solvers.options.update(old_options)\n",
    "\n",
    "    l = res['x'][-nB:]\n",
    "    d = res['x'][n:n+nC]\n",
    "    t = B*l + C*d\n",
    "    q = res['x'][:n]\n",
    "    p = A * q\n",
    "    r = M * q\n",
    "    e = y - r - t\n",
    "\n",
    "    return (np.array(a).ravel() for a in (r, p, t, l, d, e, obj))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T23:23:56.483626Z",
     "start_time": "2018-09-04T23:23:56.275961Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Processes E4 zipfile\n",
    "Unzip all zip files in a folder. \n",
    "Append a suffix (user_id) to the extracted files. \n",
    "Delete zip file. \n",
    "'''\n",
    "def process_e4(folder, verbose = True):\n",
    "    user = os.path.basename(folder)\n",
    "    zips = glob(folder + \"/e4*.zip\")\n",
    "    if len(zips) == 0: \n",
    "        print(\"E4 processed\")\n",
    "    for z in zips:\n",
    "        zip_full_path = os.path.realpath(z)\n",
    "        directory_to_extract_to = os.path.dirname(zip_full_path)\n",
    "        zip_ref = zipfile.ZipFile(zip_full_path, 'r')\n",
    "        zip_ref.extractall(directory_to_extract_to)\n",
    "        files = zip_ref.namelist()\n",
    "        if verbose: \n",
    "            print(\"Unzipping\", z)\n",
    "        zip_ref.close()\n",
    "        for file in files: \n",
    "            prefixed = file.replace(\".\", \"_\"+user + \".\").lower()\n",
    "            src = directory_to_extract_to + \"/\" + file\n",
    "            dest = directory_to_extract_to + \"/\" + prefixed\n",
    "            if verbose: \n",
    "                print(\"Renaming\", os.path.basename(src), os.path.basename(dest))\n",
    "            os.rename(src, dest)\n",
    "        os.remove(zip_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-05T16:45:57.042126Z",
     "start_time": "2018-09-05T16:45:57.022302Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_log(folder):\n",
    "    user = os.path.basename(folder)\n",
    "    logs = glob(folder + \"/log*.json\")\n",
    "    if len(logs) == 0:\n",
    "        print(\"ERROR: No log found for\", user)\n",
    "    else:\n",
    "        for log in logs:\n",
    "            logname, ext = os.path.basename(log).split('.')\n",
    "            if len(logname.split('_')) == 3:\n",
    "                filetype, user_id, starttime = logname.split('_')\n",
    "                if user_id != user: \n",
    "                    print(\"ERROR: Log is incorrectly labeled/in wrong folder\")\n",
    "                with open(log, 'r+') as f: \n",
    "                    data = json.load(f)\n",
    "#                     if data[0]['type'] != \"logstart\":\n",
    "#                         data.insert(0, {\n",
    "#                             'type': 'logstart', \n",
    "#                             'time': int(starttime)\n",
    "#                         })\n",
    "#                         print(\"Appending start time to\", log)\n",
    "                    f.seek(0)\n",
    "                    info = {}\n",
    "                    info['data'] = data\n",
    "                    json.dump(info, f)\n",
    "                    f.truncate()\n",
    "                    print(\"Starttime appended to log file\")\n",
    "            \n",
    "                name = \".\".join([\"_\".join([filetype, user_id]), ext])\n",
    "                name = os.path.join(os.path.dirname(log), name) \n",
    "                os.rename(log, name)\n",
    "            print(\"Log processed.\", log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T22:15:41.336258Z",
     "start_time": "2018-09-04T22:15:41.313256Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Moves video to a data directory and makes a metadata json with paths to all video resources.\n",
    "'''\n",
    "def process_videos(folder):\n",
    "    data_dir = folder + \"/data\"\n",
    "    user = os.path.basename(folder)\n",
    "    if not os.path.exists(data_dir):\n",
    "       print(\"Making data directory\")\n",
    "       os.makedirs(data_dir)\n",
    "        \n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.mp4\"))\n",
    "    files.extend(glob(folder + \"/*.mov\"))\n",
    "    for video in files: \n",
    "        src = video\n",
    "        name = os.path.basename(src)\n",
    "        os.rename(src, os.path.join(data_dir, name))\n",
    "    \n",
    "    data = get_video_metadata(data_dir)\n",
    "    save_jsonfile(os.path.join(folder, \"videometadata_111.json\"), data)\n",
    "    \n",
    "def get_video_metadata(folder):\n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.mp4\"))\n",
    "    files.extend(glob(folder + \"/*.mov\"))\n",
    "    \n",
    "    data = {}\n",
    "    for video in files: \n",
    "        name = os.path.basename(video)\n",
    "        file_type = name.split(\"_\")[0]\n",
    "        if not file_type in data:\n",
    "            data[file_type] = {}\n",
    "        if \"opt\" in video:            \n",
    "            data[file_type]['opt'] = video\n",
    "        else:\n",
    "            data[file_type]['raw']= video\n",
    "    return data\n",
    "#        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empatica Biosignals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T22:15:42.956981Z",
     "start_time": "2018-09-04T22:15:42.947908Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Ensures all files have user_id as the second argument. \n",
    "'''\n",
    "def process_bio(folder):\n",
    "    user = os.path.basename(folder)\n",
    "    files = glob(folder + \"/*.csv\")\n",
    "    for f in files:\n",
    "        info = extract_info(f)\n",
    "        directory = os.path.dirname(f)\n",
    "        filename, ext = os.path.basename(f).split('.')\n",
    "        filetype, user_id = filename.split(\"_\")\n",
    "        data = dict(info, **E4_MANIFEST[filetype])\n",
    "        data['user_id'] = int(user)\n",
    "        data['session_id'] = SESSION_ID\n",
    "        save_jsonfile(os.path.join(directory, filetype + \"_\" + user_id + \".json\"), data)\n",
    "        os.remove(f)\n",
    "    print(\"Processed bio csv files to json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T22:15:43.694768Z",
     "start_time": "2018-09-04T22:15:43.676863Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_info(file):\n",
    "    info = {}\n",
    "    name = os.path.basename(file).split('.')[0].split(\"_\")[0]   \n",
    "    info['name'] = name\n",
    "    with open(file) as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        if name != \"tags\":\n",
    "            info['timestamp'] = int(float(next(csvreader)[0]))\n",
    "            info['sampling_rate'] = int(float(next(csvreader)[0]))\n",
    "        if name == \"ibi\":\n",
    "            data = []\n",
    "            for row in csvreader:\n",
    "                data.append([float(row[0]), float(row[1])])\n",
    "            info['data'] = data\n",
    "        elif name == \"acc\":\n",
    "            data = []\n",
    "            for row in csvreader:\n",
    "                data.append([float(row[0]), float(row[1]), float(row[2])])\n",
    "            info['data'] = data\n",
    "        else: \n",
    "            data = []\n",
    "            for row in csvreader:\n",
    "                data.append(float(row[0]))\n",
    "            info['data'] = data\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T22:15:44.497168Z",
     "start_time": "2018-09-04T22:15:44.487005Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Ensures all files have user_id as the second argument. \n",
    "'''\n",
    "def process_filenames(folder):\n",
    "    user = os.path.basename(folder)\n",
    "    files = glob(folder + \"/*\")\n",
    "    for f in files:\n",
    "        if os.path.isfile(f):\n",
    "            directory = os.path.dirname(f)\n",
    "            name, ext = os.path.basename(f).split(\".\")\n",
    "            filetype, user_id = name.split(\"_\")\n",
    "            if user != user_id: \n",
    "                print(\"Invalid\", f, 'expecting', user, 'but got', user_id)\n",
    "            lname = \".\".join([\"_\".join([filetype.lower(), user_id]), ext])\n",
    "            src = f\n",
    "            dst = os.path.join(directory, lname) \n",
    "            os.rename(src, dst)\n",
    "    print(\"Processed files for correct naming conventions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T22:15:45.136706Z",
     "start_time": "2018-09-04T22:15:45.127475Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_images(folder): \n",
    "    data_dir = folder + \"/data\"\n",
    "    user = os.path.basename(folder)\n",
    "    if not os.path.exists(data_dir):\n",
    "       print(\"Making data directory\")\n",
    "       os.makedirs(data_dir)\n",
    "        \n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.jpg\"))\n",
    "    files.extend(glob(folder + \"/*.png\"))\n",
    "    for img in files: \n",
    "        src = img\n",
    "        name = os.path.basename(src)\n",
    "        os.rename(src, os.path.join(data_dir, name))\n",
    "    \n",
    "    data = get_img_metadata(data_dir)\n",
    "    save_jsonfile(os.path.join(folder, \"imagemetadata_111.json\"), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T22:15:45.689799Z",
     "start_time": "2018-09-04T22:15:45.682206Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img_metadata(folder):\n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.png\"))\n",
    "    files.extend(glob(folder + \"/*.jpg\"))\n",
    "    \n",
    "    data = {}\n",
    "    for img in files: \n",
    "        name = os.path.basename(img)\n",
    "        file_type = name.split(\"_\")[0]\n",
    "        if not file_type in data:\n",
    "            data[file_type] = []    \n",
    "        data[file_type].append(img)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T22:15:46.785120Z",
     "start_time": "2018-09-04T22:15:46.776000Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_notes(folder): \n",
    "    data_dir = folder + \"/data\"\n",
    "    user = os.path.basename(folder)\n",
    "    if not os.path.exists(data_dir):\n",
    "       print(\"Making data directory\")\n",
    "       os.makedirs(data_dir)\n",
    "        \n",
    "    files = glob(folder + \"/*.txt\")\n",
    "    for note in files: \n",
    "        src = note\n",
    "        name = os.path.basename(src)\n",
    "        os.rename(src, os.path.join(data_dir, name))\n",
    "    \n",
    "    data = get_notes_metadata(data_dir)\n",
    "    save_jsonfile(os.path.join(folder, \"notesmetadata_111.json\"), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T22:15:47.399232Z",
     "start_time": "2018-09-04T22:15:47.392262Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_notes_metadata(folder):\n",
    "    files = glob(folder + \"/*.txt\")\n",
    "\n",
    "    \n",
    "    data = {}\n",
    "    for note in files: \n",
    "        name = os.path.basename(note)\n",
    "        file_type = name.split(\"_\")[0]\n",
    "        if not file_type in data:\n",
    "            data[file_type] = []    \n",
    "        data[file_type].append(note)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-04T23:23:44.051646Z",
     "start_time": "2018-09-04T23:23:44.046084Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NUKE THE UNOPTIMIZED VIDEO FILES\n",
    "def nuke_vids(folder):\n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.mp4\"))\n",
    "    files.extend(glob(folder + \"/*.mov\"))\n",
    "    \n",
    "    for video in files: \n",
    "        if not \"opt\" in video:\n",
    "            os.remove(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:35:33.536062Z",
     "start_time": "2018-09-02T11:35:33.533139Z"
    }
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-05T16:46:00.499433Z",
     "start_time": "2018-09-05T16:46:00.247750Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E4 processed\n",
      "Starttime appended to log file\n",
      "Log processed. irb/111/log_111_1929292.json\n",
      "Processed bio csv files to json.\n",
      "File saved! irb/111/videometadata_111.json\n",
      "File saved! irb/111/imagemetadata_111.json\n",
      "File saved! irb/111/notesmetadata_111.json\n",
      "Processed files for correct naming conventions.\n"
     ]
    }
   ],
   "source": [
    "data = filter(os.path.isdir, glob(DATA_ROOT + \"/*\"))\n",
    "for user_folder in data:\n",
    "    user = os.path.basename(user_folder)\n",
    "    process_e4(user_folder)\n",
    "    process_log(user_folder)\n",
    "    process_bio(user_folder)\n",
    "    process_videos(user_folder)\n",
    "    process_images(user_folder)\n",
    "    process_notes(user_folder)\n",
    "    process_filenames(user_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-05T16:35:45.427208Z",
     "start_time": "2018-09-05T16:35:45.423139Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "data = filter(os.path.isdir, glob(DATA_ROOT + \"/*\"))\n",
    "for user_folder in data: \n",
    "    nuke_vids(os.path.join(user_folder, 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
