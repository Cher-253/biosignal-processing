{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:41:32.453749Z",
     "start_time": "2018-09-21T03:41:32.404919Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, os, csv, platform, zipfile, datetime, time\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "DATA_ROOT = \"irb\"\n",
    "SESSION_ID = \"CHI2019\"\n",
    "\n",
    "E4_MANIFEST = {\n",
    "    'phasic': {\n",
    "        'name': \"Phasic GSR\",\n",
    "        'description': \"Event-specific EDA\"\n",
    "    },\n",
    "    'smna':{\n",
    "        'name': \"SMNA\",\n",
    "        'description': \"sparse SMNA driver of phasic component\"\n",
    "    },\n",
    "    'textchunk': {\n",
    "        'name': \"Text Chunk\",\n",
    "        'description': \"Values correspond to typing behaviors.  # of consecutive characters typed.\"\n",
    "    },\n",
    "    'temp': {\n",
    "        'name': \"Temperature\",\n",
    "        'description': \"Data from temperature sensor expressed degrees on the Celsius (°C) scale.\",\n",
    "        'unit': \"celsius\"\n",
    "    },\n",
    "    'tags':{\n",
    "        'name': \"Tags\",\n",
    "        'description': \"Event mark times. Each row corresponds to a physical button press on the device; the same time as the status LED is first illuminated. The time is expressed as a unix timestamp in UTC and it is synchronized with initial time of the session indicated in the related data files from the corresponding session.\"\n",
    "    },\n",
    "    'acc':{\n",
    "        'description': \"Data from 3-axis accelerometer sensor. The accelerometer is configured to measure acceleration in the range [-2g, 2g]. Therefore the unit in this file is 1/64g. Data from x, y, and z axis are respectively in first, second, and third column.\",\n",
    "        'name': \"3-Axis Accelerometer\",\n",
    "        'unit': \"1/64g\"\n",
    "    }, \n",
    "    'eda':{\n",
    "        'description': \"Data from the electrodermal activity sensor expressed as microsiemens (μS).\", \n",
    "        'name':\"Electrodermal Activity\",\n",
    "        'unit': \"μS\"\n",
    "    }, \n",
    "    'bvp':{\n",
    "        'name': \"Blood Volume Pulse (BVP) from PPG\",\n",
    "        'description': \"Data from photoplethysmograph.\",\n",
    "    }, \n",
    "    'ibi':{\n",
    "        'name': \"IBI\", \n",
    "        'description': \"Time between individuals heart beats extracted from the BVP signal. No sample rate is needed for this file. The first column is the time (respect to the initial time) of the detected inter-beat interval expressed in seconds (s). The second column is the duration in seconds (s) of the detected inter-beat interval (i.e., the distance in seconds from the previous beat).\"\n",
    "    },\n",
    "    'hr':{\n",
    "        'name': \"Heart rate\", \n",
    "        'description': \"Average heart rate extracted from the BVP signal.The first row is the initial time of the session expressed as unix timestamp in UTC. The second row is the sample rate expressed in Hz.\"\n",
    "    }, \n",
    "    'kinnunen':{\n",
    "        'name': \"Kinnunen codes\", \n",
    "        'description': \"From Kinnunen et al.; self-efficacy assessments x emotions; used as a closed-coding system. Addition of a failure stage.\"\n",
    "    }, \n",
    "    'a':{\n",
    "        'name': \"soldering acc\"\n",
    "    }, \n",
    "    'g':{\n",
    "         'name': \"gyro\"\n",
    "    }, \n",
    "    'm':{\n",
    "        'name': 'magnetometer'\n",
    "    }\n",
    "}\n",
    "def save_jsonfile(name, data):\n",
    "    with open(name, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "    print(\"File saved!\", name)\n",
    "    \n",
    "def gen_save_file(folder, feature):\n",
    "    user = os.path.basename(folder)\n",
    "    return os.path.join(folder, feature + \"_\" + user + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:05.968599Z",
     "start_time": "2018-09-21T03:44:05.953282Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Processes E4 zipfile\n",
    "Unzip all zip files in a folder. \n",
    "Append a suffix (user_id) to the extracted files. \n",
    "Delete zip file. \n",
    "'''\n",
    "def process_e4(folder, verbose = True):\n",
    "    user = os.path.basename(folder)\n",
    "    zips = glob(folder + \"/e4*.zip\")\n",
    "    if len(zips) == 0: \n",
    "        print(\"E4 processed\")\n",
    "    for z in zips:\n",
    "        zip_full_path = os.path.realpath(z)\n",
    "        directory_to_extract_to = os.path.dirname(zip_full_path)\n",
    "        zip_ref = zipfile.ZipFile(zip_full_path, 'r')\n",
    "        zip_ref.extractall(directory_to_extract_to)\n",
    "        files = zip_ref.namelist()\n",
    "        if verbose: \n",
    "            print(\"Unzipping\", z)\n",
    "        zip_ref.close()\n",
    "        for file in files: \n",
    "            print(file)\n",
    "            prefixed = file.replace(\".\", \"_\"+user + \".\").lower()\n",
    "            src = directory_to_extract_to + \"/\" + file\n",
    "            dest = directory_to_extract_to + \"/\" + prefixed\n",
    "            if verbose: \n",
    "                print(\"Renaming\", os.path.basename(src), os.path.basename(dest))\n",
    "            os.rename(src, dest)\n",
    "        os.remove(zip_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:06.480772Z",
     "start_time": "2018-09-21T03:44:06.461044Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_log(folder):\n",
    "    user = os.path.basename(folder)\n",
    "    logs = glob(folder + \"/log*.json\")\n",
    "    if len(logs) == 0:\n",
    "        print(\"ERROR: No log found for\", user)\n",
    "    else:\n",
    "        for log in logs:\n",
    "            logname, ext = os.path.basename(log).split('.')\n",
    "            if len(logname.split('_')) == 3:\n",
    "                filetype, user_id, starttime = logname.split('_')\n",
    "                if user_id != user: \n",
    "                    print(\"ERROR: Log is incorrectly labeled/in wrong folder\")\n",
    "                with open(log, 'r+') as f: \n",
    "                    data = json.load(f)\n",
    "                    for i, d in enumerate(data): \n",
    "                        data[i]['time'] = float(d['time'])/ 1000\n",
    "                    f.seek(0)\n",
    "                    info = {}\n",
    "                    info['data'] = data\n",
    "                    json.dump(info, f)\n",
    "                    f.truncate()\n",
    "                    print(\"Starttime appended to log file\")\n",
    "            \n",
    "                name = \".\".join([\"_\".join([filetype, user_id]), ext])\n",
    "                name = os.path.join(os.path.dirname(log), name) \n",
    "                os.rename(log, name)\n",
    "            print(\"Log processed.\", log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:06.980691Z",
     "start_time": "2018-09-21T03:44:06.955104Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Moves video to a data directory and makes a metadata json with paths to all video resources.\n",
    "'''\n",
    "def process_videos(folder):\n",
    "    data_dir = folder + \"/data\"\n",
    "    user = os.path.basename(folder)\n",
    "    if not os.path.exists(data_dir):\n",
    "       print(\"Making data directory\")\n",
    "       os.makedirs(data_dir)\n",
    "        \n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.mp4\"))\n",
    "    files.extend(glob(folder + \"/*.m4v\"))\n",
    "    files.extend(glob(folder + \"/*.mov\"))\n",
    "    for video in files: \n",
    "        src = video\n",
    "        name = os.path.basename(src)\n",
    "        os.rename(src, os.path.join(data_dir, name))\n",
    "    \n",
    "    data = get_video_metadata(data_dir)\n",
    "    save_jsonfile(os.path.join(folder, \"videometadata_\"+user+\".json\"), data)\n",
    "    \n",
    "def get_video_metadata(folder):\n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.mp4\"))\n",
    "    files.extend(glob(folder + \"/*.mov\"))\n",
    "    \n",
    "    data = {}\n",
    "    for video in files: \n",
    "        name = os.path.basename(video)\n",
    "        file_type = name.split(\"_\")[0]\n",
    "        if not file_type in data:\n",
    "            data[file_type] = {}\n",
    "        if \"opt\" in video:            \n",
    "            data[file_type]['opt'] = video\n",
    "        else:\n",
    "            data[file_type]['raw']= video\n",
    "    return data\n",
    "#        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T21:01:31.632892Z",
     "start_time": "2018-09-17T21:01:31.628867Z"
    }
   },
   "source": [
    "## Soldering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:07.649903Z",
     "start_time": "2018-09-21T03:44:07.610673Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_solder_info(folder, file):\n",
    "    info = {}\n",
    "    name = os.path.basename(file).split('.')[0].split(\"_\")[0]   \n",
    "    info['name'] = name\n",
    "    with open(file) as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        r = next(csvreader)\n",
    "        info['timestamp'] = int(float(r[0]))\n",
    "        info['end_timestamp'] = int(float(r[1]))\n",
    "        info['elapsed_time'] = int(float(r[2]))\n",
    "        \n",
    "        print(time.ctime(info['timestamp']))\n",
    "        print(time.ctime(info['end_timestamp']))\n",
    "        print(info['elapsed_time']/60)\n",
    "        \n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "        \n",
    "        for row in csvreader:\n",
    "            x.append(float(row[0]))\n",
    "            y.append(float(row[1]))\n",
    "            z.append(float(row[2]))\n",
    "        \n",
    "        channels = {'x':x, 'y':y, 'z':z}\n",
    "        \n",
    "        for c in channels:\n",
    "            feature = name + \"-\" + c\n",
    "            \n",
    "            d = dict(E4_MANIFEST[name], **info)\n",
    "            d[\"name\"] = feature\n",
    "            d[\"data\"] = channels[c]\n",
    "            \n",
    "            savefile = gen_save_file(folder, feature)\n",
    "            save_jsonfile(savefile, d)\n",
    "     \n",
    "    return info\n",
    "def process_soldering(folder):\n",
    "    user = os.path.basename(folder)\n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/a_*.csv\"))\n",
    "    files.extend(glob(folder + \"/g_*.csv\"))\n",
    "    files.extend(glob(folder + \"/m_*.csv\"))\n",
    "    for f in files:\n",
    "        info = extract_solder_info(folder,f)\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:07.903026Z",
     "start_time": "2018-09-21T03:44:07.899902Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_soldering(\"irb/722\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empatica Biosignals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:53.318795Z",
     "start_time": "2018-09-21T03:44:53.305881Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Ensures all files have user_id as the second argument. \n",
    "'''\n",
    "def process_bio(folder):\n",
    "    user = os.path.basename(folder)\n",
    "    files = glob(folder + \"/*.csv\")\n",
    "    print(files)\n",
    "    for f in files:\n",
    "        \n",
    "        directory = os.path.dirname(f)\n",
    "        filename, ext = os.path.basename(f).split('.')\n",
    "        filetype, user_id = filename.split(\"_\")\n",
    "        if filetype == \"code\":\n",
    "            continue\n",
    "        print(\"INFO\", f)\n",
    "        info = extract_info(f)\n",
    "        \n",
    "        data = dict(info, **E4_MANIFEST[filetype])\n",
    "        data['user_id'] = int(user)\n",
    "        data['session_id'] = SESSION_ID\n",
    "        save_jsonfile(os.path.join(directory, filetype + \"_\" + user_id + \".json\"), data)\n",
    "        os.remove(f)\n",
    "    print(\"Processed bio csv files to json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:09.139331Z",
     "start_time": "2018-09-21T03:44:09.122415Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_info(file):\n",
    "    info = {}\n",
    "    name = os.path.basename(file).split('.')[0].split(\"_\")[0]   \n",
    "    info['name'] = name\n",
    "    with open(file) as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        if name != \"tags\":\n",
    "            info['timestamp'] = int(float(next(csvreader)[0]))\n",
    "            info['sampling_rate'] = int(float(next(csvreader)[0]))\n",
    "        if name == \"ibi\":\n",
    "            data = []\n",
    "            for row in csvreader:\n",
    "                data.append([float(row[0]), float(row[1])])\n",
    "            info['data'] = data\n",
    "        elif name == \"acc\":\n",
    "            data = []\n",
    "            for row in csvreader:\n",
    "                data.append([float(row[0]), float(row[1]), float(row[2])])\n",
    "            info['data'] = data\n",
    "        else: \n",
    "            data = []\n",
    "            for row in csvreader:\n",
    "                data.append(float(row[0]))\n",
    "            info['data'] = data\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:09.365765Z",
     "start_time": "2018-09-21T03:44:09.355080Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Ensures all files have user_id as the second argument. \n",
    "'''\n",
    "def process_filenames(folder):\n",
    "    user = os.path.basename(folder)\n",
    "    files = glob(folder + \"/*\")\n",
    "    for f in files:\n",
    "        if os.path.isfile(f):\n",
    "            directory = os.path.dirname(f)\n",
    "            name, ext = os.path.basename(f).split(\".\")\n",
    "            print(name)\n",
    "            filetype, user_id = name.split(\"_\")\n",
    "            if user != user_id: \n",
    "                print(\"Invalid\", f, 'expecting', user, 'but got', user_id)\n",
    "            lname = \".\".join([\"_\".join([filetype.lower(), user_id]), ext])\n",
    "            src = f\n",
    "            dst = os.path.join(directory, lname) \n",
    "            os.rename(src, dst)\n",
    "    print(\"Processed files for correct naming conventions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:09.592331Z",
     "start_time": "2018-09-21T03:44:09.582236Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_images(folder): \n",
    "    data_dir = folder + \"/data\"\n",
    "    user = os.path.basename(folder)\n",
    "    if not os.path.exists(data_dir):\n",
    "       print(\"Making data directory\")\n",
    "       os.makedirs(data_dir)\n",
    "        \n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.jpg\"))\n",
    "    files.extend(glob(folder + \"/*.JPG\"))\n",
    "    files.extend(glob(folder + \"/*.png\"))\n",
    "    for img in files: \n",
    "        src = img\n",
    "        name = os.path.basename(src)\n",
    "        os.rename(src, os.path.join(data_dir, name))\n",
    "    \n",
    "    data = get_img_metadata(data_dir)\n",
    "    save_jsonfile(os.path.join(folder, \"imagemetadata_\"+user+\".json\"), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:09.883576Z",
     "start_time": "2018-09-21T03:44:09.876721Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img_metadata(folder):\n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.png\"))\n",
    "    files.extend(glob(folder + \"/*.jpg\"))\n",
    "    \n",
    "    data = {}\n",
    "    for img in files: \n",
    "        name = os.path.basename(img)\n",
    "        file_type = name.split(\"_\")[0]\n",
    "        if not file_type in data:\n",
    "            data[file_type] = []    \n",
    "        data[file_type].append(img)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:10.331467Z",
     "start_time": "2018-09-21T03:44:10.323317Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_notes(folder): \n",
    "    data_dir = folder + \"/data\"\n",
    "    user = os.path.basename(folder)\n",
    "    if not os.path.exists(data_dir):\n",
    "       print(\"Making data directory\")\n",
    "       os.makedirs(data_dir)\n",
    "        \n",
    "    files = glob(folder + \"/*.txt\")\n",
    "    for note in files: \n",
    "        src = note\n",
    "        name = os.path.basename(src)\n",
    "        os.rename(src, os.path.join(data_dir, name))\n",
    "    \n",
    "    data = get_notes_metadata(data_dir)\n",
    "    save_jsonfile(os.path.join(folder, \"notesmetadata_\"+user+\".json\"), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:10.784183Z",
     "start_time": "2018-09-21T03:44:10.777882Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_notes_metadata(folder):\n",
    "    files = glob(folder + \"/*.txt\")\n",
    "    data = {}\n",
    "    for note in files: \n",
    "        name = os.path.basename(note)\n",
    "        file_type = name.split(\"_\")[0]\n",
    "        if not file_type in data:\n",
    "            data[file_type] = []    \n",
    "        data[file_type].append(note)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:44:11.356395Z",
     "start_time": "2018-09-21T03:44:11.349905Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NUKE THE UNOPTIMIZED VIDEO FILES\n",
    "def nuke_vids(folder):\n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.mp4\"))\n",
    "    files.extend(glob(folder + \"/*.mov\"))\n",
    "    files.extend(glob(folder + \"/*.m4v\"))\n",
    "    \n",
    "    print(files)\n",
    "    for video in files: \n",
    "        if not \"opt\" in video:\n",
    "            os.remove(video)\n",
    "            \n",
    "    for f in glob(user_folder + \"/*.docx\"):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:35:33.536062Z",
     "start_time": "2018-09-02T11:35:33.533139Z"
    }
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T03:45:10.814724Z",
     "start_time": "2018-09-21T03:45:10.164539Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E4 processed\n",
      "['irb/1000/eda_1000.csv', 'irb/1000/temp_1000.csv', 'irb/1000/acc_1000.csv', 'irb/1000/hr_1000.csv', 'irb/1000/bvp_1000.csv']\n",
      "INFO irb/1000/eda_1000.csv\n",
      "File saved! irb/1000/eda_1000.json\n",
      "INFO irb/1000/temp_1000.csv\n",
      "File saved! irb/1000/temp_1000.json\n",
      "INFO irb/1000/acc_1000.csv\n",
      "File saved! irb/1000/acc_1000.json\n",
      "INFO irb/1000/hr_1000.csv\n",
      "File saved! irb/1000/hr_1000.json\n",
      "INFO irb/1000/bvp_1000.csv\n",
      "File saved! irb/1000/bvp_1000.json\n",
      "Processed bio csv files to json.\n",
      "Making data directory\n",
      "File saved! irb/1000/videometadata_1000.json\n",
      "File saved! irb/1000/imagemetadata_1000.json\n",
      "File saved! irb/1000/notesmetadata_1000.json\n",
      "g-x_1000\n",
      "a-y_1000\n",
      "eda_1000\n",
      "videometadata_1000\n",
      "notesmetadata_1000\n",
      "m-y_1000\n",
      "g-y_1000\n",
      "a-x_1000\n",
      "imagemetadata_1000\n",
      "hr_1000\n",
      "acc_1000\n",
      "m-x_1000\n",
      "tags_1000\n",
      "g-z_1000\n",
      "temp_1000\n",
      "a-z_1000\n",
      "m-z_1000\n",
      "bvp_1000\n",
      "Processed files for correct naming conventions.\n"
     ]
    }
   ],
   "source": [
    "data = filter(os.path.isdir, glob(DATA_ROOT + \"/*\"))\n",
    "for user_folder in data:\n",
    "    user = os.path.basename(user_folder)\n",
    "    if user == \"datasets\":\n",
    "        continue\n",
    "    process_soldering(user_folder)\n",
    "    process_e4(user_folder)\n",
    "#     process_log(user_folder)\n",
    "    process_bio(user_folder)\n",
    "    process_videos(user_folder)\n",
    "    process_images(user_folder)\n",
    "    process_notes(user_folder)\n",
    "    process_filenames(user_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T08:21:33.820018Z",
     "start_time": "2018-09-20T15:21:33.347Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "data = filter(os.path.isdir, glob(DATA_ROOT + \"/*\"))\n",
    "for user_folder in data: \n",
    "    nuke_vids(os.path.join(user_folder, 'data'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
