{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:37:56.246107Z",
     "start_time": "2018-09-08T14:37:56.237509Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, os, csv, platform, zipfile, datetime\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "DATA_ROOT = \"irb\"\n",
    "SESSION_ID = \"CHI2019\"\n",
    "\n",
    "def save_jsonfile(name, data):\n",
    "    with open(name, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "    print(\"File saved!\", name)\n",
    "    \n",
    "def gen_save_file(folder, feature):\n",
    "    user = os.path.basename(folder)\n",
    "    return os.path.join(folder, feature + \"_\" + user + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:37:56.427489Z",
     "start_time": "2018-09-08T14:37:56.412726Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Processes E4 zipfile\n",
    "Unzip all zip files in a folder. \n",
    "Append a suffix (user_id) to the extracted files. \n",
    "Delete zip file. \n",
    "'''\n",
    "def process_e4(folder, verbose = True):\n",
    "    user = os.path.basename(folder)\n",
    "    zips = glob(folder + \"/e4*.zip\")\n",
    "    if len(zips) == 0: \n",
    "        print(\"E4 processed\")\n",
    "    for z in zips:\n",
    "        zip_full_path = os.path.realpath(z)\n",
    "        directory_to_extract_to = os.path.dirname(zip_full_path)\n",
    "        zip_ref = zipfile.ZipFile(zip_full_path, 'r')\n",
    "        zip_ref.extractall(directory_to_extract_to)\n",
    "        files = zip_ref.namelist()\n",
    "        if verbose: \n",
    "            print(\"Unzipping\", z)\n",
    "        zip_ref.close()\n",
    "        for file in files: \n",
    "            prefixed = file.replace(\".\", \"_\"+user + \".\").lower()\n",
    "            src = directory_to_extract_to + \"/\" + file\n",
    "            dest = directory_to_extract_to + \"/\" + prefixed\n",
    "            if verbose: \n",
    "                print(\"Renaming\", os.path.basename(src), os.path.basename(dest))\n",
    "            os.rename(src, dest)\n",
    "        os.remove(zip_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:37:56.610135Z",
     "start_time": "2018-09-08T14:37:56.591625Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_log(folder):\n",
    "    user = os.path.basename(folder)\n",
    "    logs = glob(folder + \"/log*.json\")\n",
    "    if len(logs) == 0:\n",
    "        print(\"ERROR: No log found for\", user)\n",
    "    else:\n",
    "        for log in logs:\n",
    "            logname, ext = os.path.basename(log).split('.')\n",
    "            if len(logname.split('_')) == 3:\n",
    "                filetype, user_id, starttime = logname.split('_')\n",
    "                if user_id != user: \n",
    "                    print(\"ERROR: Log is incorrectly labeled/in wrong folder\")\n",
    "                with open(log, 'r+') as f: \n",
    "                    data = json.load(f)\n",
    "                    for i, d in enumerate(data): \n",
    "                        data[i]['time'] = float(d['time'])/ 1000\n",
    "                    f.seek(0)\n",
    "                    info = {}\n",
    "                    info['data'] = data\n",
    "                    json.dump(info, f)\n",
    "                    f.truncate()\n",
    "                    print(\"Starttime appended to log file\")\n",
    "            \n",
    "                name = \".\".join([\"_\".join([filetype, user_id]), ext])\n",
    "                name = os.path.join(os.path.dirname(log), name) \n",
    "                os.rename(log, name)\n",
    "            print(\"Log processed.\", log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:37:57.113700Z",
     "start_time": "2018-09-08T14:37:57.089828Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Moves video to a data directory and makes a metadata json with paths to all video resources.\n",
    "'''\n",
    "def process_videos(folder):\n",
    "    data_dir = folder + \"/data\"\n",
    "    user = os.path.basename(folder)\n",
    "    if not os.path.exists(data_dir):\n",
    "       print(\"Making data directory\")\n",
    "       os.makedirs(data_dir)\n",
    "        \n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.mp4\"))\n",
    "    files.extend(glob(folder + \"/*.mov\"))\n",
    "    for video in files: \n",
    "        src = video\n",
    "        name = os.path.basename(src)\n",
    "        os.rename(src, os.path.join(data_dir, name))\n",
    "    \n",
    "    data = get_video_metadata(data_dir)\n",
    "    save_jsonfile(os.path.join(folder, \"videometadata_\"+user+\".json\"), data)\n",
    "    \n",
    "def get_video_metadata(folder):\n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.mp4\"))\n",
    "    files.extend(glob(folder + \"/*.mov\"))\n",
    "    \n",
    "    data = {}\n",
    "    for video in files: \n",
    "        name = os.path.basename(video)\n",
    "        file_type = name.split(\"_\")[0]\n",
    "        if not file_type in data:\n",
    "            data[file_type] = {}\n",
    "        if \"opt\" in video:            \n",
    "            data[file_type]['opt'] = video\n",
    "        else:\n",
    "            data[file_type]['raw']= video\n",
    "    return data\n",
    "#        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empatica Biosignals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:37:57.844982Z",
     "start_time": "2018-09-08T14:37:57.834665Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Ensures all files have user_id as the second argument. \n",
    "'''\n",
    "def process_bio(folder):\n",
    "    user = os.path.basename(folder)\n",
    "    files = glob(folder + \"/*.csv\")\n",
    "    for f in files:\n",
    "        info = extract_info(f)\n",
    "        directory = os.path.dirname(f)\n",
    "        filename, ext = os.path.basename(f).split('.')\n",
    "        filetype, user_id = filename.split(\"_\")\n",
    "        data = dict(info, **E4_MANIFEST[filetype])\n",
    "        data['user_id'] = int(user)\n",
    "        data['session_id'] = SESSION_ID\n",
    "        save_jsonfile(os.path.join(directory, filetype + \"_\" + user_id + \".json\"), data)\n",
    "        os.remove(f)\n",
    "    print(\"Processed bio csv files to json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:37:58.358811Z",
     "start_time": "2018-09-08T14:37:58.340250Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_info(file):\n",
    "    info = {}\n",
    "    name = os.path.basename(file).split('.')[0].split(\"_\")[0]   \n",
    "    info['name'] = name\n",
    "    with open(file) as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        if name != \"tags\":\n",
    "            info['timestamp'] = int(float(next(csvreader)[0]))\n",
    "            info['sampling_rate'] = int(float(next(csvreader)[0]))\n",
    "        if name == \"ibi\":\n",
    "            data = []\n",
    "            for row in csvreader:\n",
    "                data.append([float(row[0]), float(row[1])])\n",
    "            info['data'] = data\n",
    "        elif name == \"acc\":\n",
    "            data = []\n",
    "            for row in csvreader:\n",
    "                data.append([float(row[0]), float(row[1]), float(row[2])])\n",
    "            info['data'] = data\n",
    "        else: \n",
    "            data = []\n",
    "            for row in csvreader:\n",
    "                data.append(float(row[0]))\n",
    "            info['data'] = data\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:37:58.789407Z",
     "start_time": "2018-09-08T14:37:58.779093Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Ensures all files have user_id as the second argument. \n",
    "'''\n",
    "def process_filenames(folder):\n",
    "    user = os.path.basename(folder)\n",
    "    files = glob(folder + \"/*\")\n",
    "    for f in files:\n",
    "        if os.path.isfile(f):\n",
    "            directory = os.path.dirname(f)\n",
    "            name, ext = os.path.basename(f).split(\".\")\n",
    "            filetype, user_id = name.split(\"_\")\n",
    "            if user != user_id: \n",
    "                print(\"Invalid\", f, 'expecting', user, 'but got', user_id)\n",
    "            lname = \".\".join([\"_\".join([filetype.lower(), user_id]), ext])\n",
    "            src = f\n",
    "            dst = os.path.join(directory, lname) \n",
    "            os.rename(src, dst)\n",
    "    print(\"Processed files for correct naming conventions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:37:59.287260Z",
     "start_time": "2018-09-08T14:37:59.277777Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_images(folder): \n",
    "    data_dir = folder + \"/data\"\n",
    "    user = os.path.basename(folder)\n",
    "    if not os.path.exists(data_dir):\n",
    "       print(\"Making data directory\")\n",
    "       os.makedirs(data_dir)\n",
    "        \n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.jpg\"))\n",
    "    files.extend(glob(folder + \"/*.png\"))\n",
    "    for img in files: \n",
    "        src = img\n",
    "        name = os.path.basename(src)\n",
    "        os.rename(src, os.path.join(data_dir, name))\n",
    "    \n",
    "    data = get_img_metadata(data_dir)\n",
    "    save_jsonfile(os.path.join(folder, \"imagemetadata_\"+user+\".json\"), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:37:59.734260Z",
     "start_time": "2018-09-08T14:37:59.727302Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img_metadata(folder):\n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.png\"))\n",
    "    files.extend(glob(folder + \"/*.jpg\"))\n",
    "    \n",
    "    data = {}\n",
    "    for img in files: \n",
    "        name = os.path.basename(img)\n",
    "        file_type = name.split(\"_\")[0]\n",
    "        if not file_type in data:\n",
    "            data[file_type] = []    \n",
    "        data[file_type].append(img)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:38:00.235473Z",
     "start_time": "2018-09-08T14:38:00.226244Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_notes(folder): \n",
    "    data_dir = folder + \"/data\"\n",
    "    user = os.path.basename(folder)\n",
    "    if not os.path.exists(data_dir):\n",
    "       print(\"Making data directory\")\n",
    "       os.makedirs(data_dir)\n",
    "        \n",
    "    files = glob(folder + \"/*.txt\")\n",
    "    for note in files: \n",
    "        src = note\n",
    "        name = os.path.basename(src)\n",
    "        os.rename(src, os.path.join(data_dir, name))\n",
    "    \n",
    "    data = get_notes_metadata(data_dir)\n",
    "    save_jsonfile(os.path.join(folder, \"notesmetadata_\"+user+\".json\"), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:38:00.730790Z",
     "start_time": "2018-09-08T14:38:00.723969Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_notes_metadata(folder):\n",
    "    files = glob(folder + \"/*.txt\")\n",
    "    data = {}\n",
    "    for note in files: \n",
    "        name = os.path.basename(note)\n",
    "        file_type = name.split(\"_\")[0]\n",
    "        if not file_type in data:\n",
    "            data[file_type] = []    \n",
    "        data[file_type].append(note)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:38:01.185296Z",
     "start_time": "2018-09-08T14:38:01.179076Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NUKE THE UNOPTIMIZED VIDEO FILES\n",
    "def nuke_vids(folder):\n",
    "    files = []\n",
    "    files.extend(glob(folder + \"/*.mp4\"))\n",
    "    files.extend(glob(folder + \"/*.mov\"))\n",
    "    \n",
    "    print(files)\n",
    "    for video in files: \n",
    "        if not \"opt\" in video:\n",
    "            os.remove(video)\n",
    "            \n",
    "    for f in glob(user_folder + \"/*.docx\"):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T11:35:33.536062Z",
     "start_time": "2018-09-02T11:35:33.533139Z"
    }
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:54:33.174096Z",
     "start_time": "2018-09-08T14:54:31.565086Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E4 processed\n",
      "Log processed. irb/111/log_111.json\n",
      "Processed bio csv files to json.\n",
      "File saved! irb/111/videometadata_111.json\n",
      "File saved! irb/111/imagemetadata_111.json\n",
      "File saved! irb/111/notesmetadata_111.json\n",
      "Processed files for correct naming conventions.\n",
      "Unzipping irb/411/e4_411_1535737565_A01B3B.zip\n",
      "Renaming ACC.csv acc_411.csv\n",
      "Renaming EDA.csv eda_411.csv\n",
      "Renaming BVP.csv bvp_411.csv\n",
      "Renaming TEMP.csv temp_411.csv\n",
      "Renaming IBI.csv ibi_411.csv\n",
      "Renaming HR.csv hr_411.csv\n",
      "Renaming info.txt info_411.txt\n",
      "Renaming tags.csv tags_411.csv\n",
      "Starttime appended to log file\n",
      "Log processed. irb/411/log_411_1535739063864.json\n",
      "File saved! irb/411/temp_411.json\n",
      "File saved! irb/411/bvp_411.json\n",
      "File saved! irb/411/hr_411.json\n",
      "File saved! irb/411/acc_411.json\n",
      "File saved! irb/411/eda_411.json\n",
      "File saved! irb/411/ibi_411.json\n",
      "File saved! irb/411/tags_411.json\n",
      "Processed bio csv files to json.\n",
      "Making data directory\n",
      "File saved! irb/411/videometadata_411.json\n",
      "File saved! irb/411/imagemetadata_411.json\n",
      "File saved! irb/411/notesmetadata_411.json\n",
      "Processed files for correct naming conventions.\n",
      "E4 processed\n",
      "ERROR: No log found for 214\n",
      "Processed bio csv files to json.\n",
      "File saved! irb/214/videometadata_214.json\n",
      "File saved! irb/214/imagemetadata_214.json\n",
      "File saved! irb/214/notesmetadata_214.json\n",
      "Processed files for correct naming conventions.\n",
      "E4 processed\n",
      "Log processed. irb/112/log_112.json\n",
      "Processed bio csv files to json.\n",
      "File saved! irb/112/videometadata_112.json\n",
      "File saved! irb/112/imagemetadata_112.json\n",
      "File saved! irb/112/notesmetadata_112.json\n",
      "Processed files for correct naming conventions.\n",
      "E4 processed\n",
      "Log processed. irb/113/log_113.json\n",
      "Processed bio csv files to json.\n",
      "File saved! irb/113/videometadata_113.json\n",
      "File saved! irb/113/imagemetadata_113.json\n",
      "File saved! irb/113/notesmetadata_113.json\n",
      "Processed files for correct naming conventions.\n"
     ]
    }
   ],
   "source": [
    "data = filter(os.path.isdir, glob(DATA_ROOT + \"/*\"))\n",
    "for user_folder in data:\n",
    "    user = os.path.basename(user_folder)\n",
    "    process_e4(user_folder)\n",
    "    process_log(user_folder)\n",
    "    process_bio(user_folder)\n",
    "    process_videos(user_folder)\n",
    "    process_images(user_folder)\n",
    "    process_notes(user_folder)\n",
    "    process_filenames(user_folder)\n",
    "    process_maxqda(user_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-08T14:54:45.483658Z",
     "start_time": "2018-09-08T14:54:45.475184Z"
    },
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['irb/111/data/interview_111_opt.mp4', 'irb/111/data/side_111_opt.mp4', 'irb/111/data/screen_111_opt.mp4']\n",
      "['irb/411/data/screen_411_opt.mp4', 'irb/411/data/interview2_411_opt.mp4', 'irb/411/data/interview1_411_opt.mp4', 'irb/411/data/side_411_opt.mp4']\n",
      "['irb/214/data/Interview_214_opt.mp4', 'irb/214/data/Side_214_opt.mp4', 'irb/214/data/Screen_214_opt.mp4']\n",
      "['irb/112/data/side_112_opt.mp4', 'irb/112/data/interview_112_opt.mp4', 'irb/112/data/screen_112_opt.mp4']\n",
      "['irb/113/data/side_113_opt.mp4', 'irb/113/data/interview_113_opt.mp4', 'irb/113/data/screen_113_opt.mp4']\n"
     ]
    }
   ],
   "source": [
    "data = filter(os.path.isdir, glob(DATA_ROOT + \"/*\"))\n",
    "for user_folder in data: \n",
    "    nuke_vids(os.path.join(user_folder, 'data'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
